{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
      "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
      "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
      "    If you step into one of those holes, you'll fall into the freezing water.\n",
      "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
      "    you navigate across the lake and retrieve the disc.\n",
      "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
      "    The surface is described using a grid like the following\n",
      "\n",
      "        SFFF\n",
      "        FHFH\n",
      "        FFFH\n",
      "        HFFG\n",
      "\n",
      "    S : starting point, safe\n",
      "    F : frozen surface, safe\n",
      "    H : hole, fall to your doom\n",
      "    G : goal, where the frisbee is located\n",
      "\n",
      "    The episode ends when you reach the goal or fall in a hole.\n",
      "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(FrozenLakeEnv.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typing\n",
    "MDP = dict\n",
    "State = int\n",
    "Action = int \n",
    "Value = float\n",
    "Policy = np.array\n",
    "Optimal_Values = np.array\n",
    "\n",
    "def show_state_space(markov_decision_process: MDP):\n",
    "    return np.array(list(markov_decision_process.keys())).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)]},\n",
       " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)]},\n",
       " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)]},\n",
       " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "# Our State Space \n",
    "print(show_state_space(env.env.P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our Action Space: \"Left\",\"Down\",\"Right\",\"Up\"\n",
    "env.env.P[3].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probability to Goal State: 0.3333333333333333 \n",
      "Goal State: 15\n",
      "Reward Function: 1.0\n",
      "Episode Over: True\n"
     ]
    }
   ],
   "source": [
    "state = 14\n",
    "action = 3\n",
    "transition_probability, next_state, reward, done = env.env.P[14][3][0]\n",
    "print(f\"Transition Probability to Goal State: {transition_probability} \\nGoal State: {next_state}\\nReward Function: {reward}\\nEpisode Over: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration \n",
    "\n",
    "*Policy* $\\pi(a | s) $: Specifies which action to take in a given state $s$. \n",
    "\n",
    "*Value Function:* Input state $s$ and outputs the value of being in that state $s$ under a particular policy $\\pi$: $V^{\\pi}: S \\rightarrow \\mathbb{R}$\n",
    "\n",
    "<p>\n",
    "$$V^{\\pi}(s) = R^{\\pi}(s) + \\gamma \\Sigma_{s' \\in S} T(s' | s, \\pi(a | s))V^{\\pi}(s')$$\n",
    "</p>\n",
    "\n",
    "Note the recursive update. This can be represented as: \n",
    "<p>\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "           V^*(s_1) \\\\\n",
    "           V^*(s_2)  \\\\\n",
    "           \\vdots \\\\\n",
    "           V^*(s_n)\n",
    "         \\end{bmatrix} = \\begin{bmatrix}\n",
    "           R^*(s_1, a_1) \\\\\n",
    "           R^*(s_2, a_2)  \\\\\n",
    "           \\vdots \\\\\n",
    "           R^*(s_n, a_n)\n",
    "         \\end{bmatrix} + \\gamma \\begin{bmatrix}\n",
    "    T(s_1, | s_1, \\pi(s_1))       & T(s_2, | s_1, \\pi(s_1)) & \\dots & T(s_n, | s_1, \\pi(s_1)) \\\\\n",
    "    T(s_1, | s_2, \\pi(s_2))       & T(s_2, | s_2, \\pi(s_2)) & \\dots & T(s_n, | s_2, \\pi(s_1)) \\\\\n",
    "    \\vdots \\ddots \\\\\n",
    "    T(s_1, | s_n, \\pi(s_n))       & T(s_2, | s_n, \\pi(s_n)) & \\dots & T(s_n, | s_n, \\pi(s_n)) \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "           V^*(s_1) \\\\\n",
    "           V^*(s_2)  \\\\\n",
    "           \\vdots \\\\\n",
    "           V^*(s_n)\n",
    "         \\end{bmatrix}\n",
    "$$\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    $$ V^* = R^* + \\gamma TV^* $$ \n",
    "</p>\n",
    "\n",
    "\n",
    "Hence, we can derive an iterative algorithm for computing the value function: \n",
    "\n",
    "**Step 1**: Initialize $t=0$, $V^*_0(s)$. \n",
    "\n",
    "**Step 2**: \n",
    "- Repeat Until Convergence. \n",
    "    - For $s \\in S$:\n",
    "        - **Update the Value Function**: $$V^{*}_t(s_t) = \\max_{a} \\Sigma_{s_{t+1}} T(s_{t+1} | s_t, a_t)(R(s_t, a_t) + \\gamma V^{*}_{t-1}(s_{t+1}))$$\n",
    "        - **Update the Policy**: $$\\pi^{*}_t(s_t) = \\arg \\max_{a}  \\Sigma_{s_{t+1}} T(s_{t+1} | s_t, a_t)(R(s_t, a_t) + \\gamma V^{*}_{t-1}(s_{t+1}))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value_of_state_action(\n",
    "    markov_decision_process: MDP,\n",
    "    state: State,\n",
    "    action: Action,\n",
    "    state_values: np.array,\n",
    "    discount: float\n",
    ") -> Value:\n",
    "    \"\"\"\n",
    "    Calculates expected value associated with being in a state and taking action\n",
    "    \"\"\"\n",
    "    qvalue = 0\n",
    "    for transition_probability, next_state, reward, done in markov_decision_process[state][action]:\n",
    "        next_state_value = 0 if done else state_values[next_state] \n",
    "        qvalue += transition_probability * (reward + discount*next_state_value)\n",
    "    return qvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert calculate_value_of_state_action(env.env.P, 14, 3, np.zeros(16), 0.99) == 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value_of_state(\n",
    "    markov_decision_process: MDP,\n",
    "    state: State,\n",
    "    state_values: np.array,\n",
    "    discount: float,\n",
    ") -> List[Value]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a list of Q[s, a] values for given state\n",
    "    \"\"\"\n",
    "    return [\n",
    "        calculate_value_of_state_action(markov_decision_process, state, action, state_values, discount) \n",
    "        for action in markov_decision_process[state]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    markov_decision_process: MDP,\n",
    "    discount: float = .99,\n",
    "    threshold: float = 0.0000001,\n",
    ") -> (Optimal_Values, Policy):\n",
    "    \"\"\"\n",
    "    Calculates the Value Function for the Markov Decision Process\n",
    "    \"\"\"\n",
    "    # Initialize the Value Function\n",
    "    V = np.zeros(len(markov_decision_process.keys()))\n",
    "    Pi = np.zeros(len(markov_decision_process.keys()))\n",
    "\n",
    "    # Repeat Until Convergence\n",
    "    while True:\n",
    "        V_prev = V.copy()\n",
    "        for state in markov_decision_process:\n",
    "            V[state] = np.max(calculate_value_of_state(env.env.P, state, V_prev, discount))\n",
    "            Pi[state] = np.argmax(calculate_value_of_state(env.env.P, state, V_prev, discount))\n",
    "\n",
    "        if np.all(np.abs(V_prev - V) < threshold):\n",
    "            break\n",
    "    return V, Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_values = np.array(\n",
    "    [0.542, 0.499, 0.471, 0.457, 0.558, 0., 0.358, 0., 0.592, 0.643, 0.615, 0., 0., 0.742, 0.863, 0.]\n",
    ")\n",
    "\n",
    "optimal_values, optimal_policy = value_iteration(env.env.P)\n",
    "assert np.sum(np.abs(optimal_values - correct_values)) <= 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration \n",
    "- **Step 1**: Choose an arbitrary policy $\\pi(a|s)$ \n",
    "- **Step 2**: Repeat until Convergence: \n",
    "    - **Policy Evaluation**: Compute the value function for $\\pi$\n",
    "        - **Approach 1**: Iterate the value function until convergence until $\\pi$\n",
    "        - **Approach 2**: Solve a linear program to compute:\n",
    "        $$V^{\\pi}_t(s_t) = \\max_{a_t} \\Sigma_{s_{t+1} \\in S} T(s_{t+1} | s_t, a_t)(R(s_t, a_t) + \\gamma V^{\\pi}_{t-1}(s_{t+1}))$$\n",
    "    - **Policy Improvement**: \n",
    "        $$\\pi_{new}(s_t) = \\arg \\max_{a_t} \\Sigma_{s_{t+1} \\in S} T(s_{t+1} | s_t, a_t)(R(s_t, a_t) + \\gamma V^{\\pi}_{t-1}(s_{t+1}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(\n",
    "    markov_decision_process: MDP, \n",
    "    policy: Policy, \n",
    "    discount: float,\n",
    "    threshold: float = 0.0000001\n",
    ") -> List[Value]:\n",
    "    \n",
    "    \"\"\"Calculates the Value Function for a Specific Policy \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the Value Function \n",
    "    V = np.zeros(len(markov_decision_process.keys()))\n",
    "    \n",
    "    # Repeat Until Convergence \n",
    "    while True: \n",
    "        V_prev = V.copy()\n",
    "        for state in markov_decision_process:\n",
    "            V[state] = calculate_value_of_state_action(markov_decision_process, state, int(policy[state]), V_prev, discount)\n",
    "        if np.all(np.abs(V_prev - V) < threshold):\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.sum(np.abs(policy_evaluation(env.env.P, optimal_policy, 0.99) - optimal_values)) <= 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Implement Policy Evaluation by Solving a Linear Program.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(\n",
    "        markov_decision_process: MDP,\n",
    "        policy: Policy,\n",
    "        values: List[Value],\n",
    "        discount: float,\n",
    ") -> Policy:\n",
    "    \n",
    "    \"\"\"Calculate the Policy Improvement Step \n",
    "    \"\"\"\n",
    "    for state in markov_decision_process:\n",
    "        policy[state] = np.argmax(calculate_value_of_state(markov_decision_process, state, values, discount))\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(markov_decision_process: MDP,\n",
    "                     discount: float = 0.99,\n",
    "                     threshold: float = 0.0001) -> Policy:\n",
    "    \n",
    "    \"\"\"Implements Policy Iteration \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a Random Policy\n",
    "    num_actions = len(markov_decision_process[0])\n",
    "    num_states = len(markov_decision_process.keys())\n",
    "    policy = np.random.choice(np.arange(num_actions), num_states)\n",
    "\n",
    "    # Repeat Until Convergence\n",
    "    while True:\n",
    "        policy_old = policy.copy()\n",
    "        value_function_policy = policy_evaluation(env.env.P, policy, discount)\n",
    "        policy = policy_improvement(env.env.P, policy, value_function_policy, discount)\n",
    "        if np.sum(np.abs(policy_old - policy)) < threshold:\n",
    "            break\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(policy_iteration(env.env.P) == optimal_policy)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
